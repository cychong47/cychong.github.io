<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>DPDK on Another blog</title>
    <link>https://cychong47.github.io/tags/dpdk/</link>
    <description>Recent content in DPDK on Another blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 20 May 2020 15:01:44 +0900</lastBuildDate>
    
	<atom:link href="https://cychong47.github.io/tags/dpdk/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>NVIDIA vRAN Solution</title>
      <link>https://cychong47.github.io/post/2020/nvidia-vran-solution/</link>
      <pubDate>Wed, 20 May 2020 15:01:44 +0900</pubDate>
      
      <guid>https://cychong47.github.io/post/2020/nvidia-vran-solution/</guid>
      <description>https://devblogs.nvidia.com/building-accelerated-5g-cloudran-at-the-edge/
Mellanox ConnectX-6 Dx SmartNIC exceeds stringent industry-standard timing specifications for eCPRI-based RANs by ensuring clock accuracy of 16ns or less
5T for 5G enables packet-based, ethernet RANs to provide precise time-stamping of packets for delivering highly accurate time references to 5G fronthaul and backhaul networks.
 5T-for-5G, or time-triggered transmission technology for telco
 https://news.developer.nvidia.com/new-real-time-smartnic-technology-5t-for-5g/
  Real-time transmission hardware acceleration: 5T-for-5G simplifies time synchronization and data transmission across servers, GPUs, radios, and baseband units in wireless network rollouts, making 5G rollouts easier and more efficient.</description>
    </item>
    
    <item>
      <title>DPDK 18.11</title>
      <link>https://cychong47.github.io/post/2019/dpdk-18-11/</link>
      <pubDate>Mon, 07 Jan 2019 14:57:59 +0900</pubDate>
      
      <guid>https://cychong47.github.io/post/2019/dpdk-18-11/</guid>
      <description>https://doc.dpdk.org/guides-18.11/rel_notes/release_18_11.html
New Features Updated the C11 memory model version of the ring library. Added changes to decrease latency for architectures using the C11 memory model version of the ring library.
On Cavium ThunderX2 platform, the changes decreased latency by 27-29% and 3-15% for MPMC and SPSC cases respectively (with 2 lcores). The real improvements may vary with the number of contending lcores and the size of the ring.
Added support for device multi-process hotplug.</description>
    </item>
    
    <item>
      <title>2nd patch submit to DPDK</title>
      <link>https://cychong47.github.io/post/2018/submit-patch-to-dpdk-with-git/</link>
      <pubDate>Sat, 21 Jul 2018 15:02:14 +0900</pubDate>
      
      <guid>https://cychong47.github.io/post/2018/submit-patch-to-dpdk-with-git/</guid>
      <description>AVX2가 지원되지 않는 머신에서 쓸데없이 ACL library 빌드할 때 AVX2를 이용해서 빌드하려는 문제를 확인했다. 지금까지 아무도 고치지 않은 게 이상하긴 한데 그래도 내가 생각한 수정 방법이 제대로 동작하는 듯 해서 패치를 한번 보내보기로 했다.
수정사항은 비교적 간단하다. ACL 라이브러리 빌드할 때 AVX2를 이용해서 빌드해야 하는 경우인지를 검사하는 코드가 lib/librte_acl/Makefile에 정의되어 있는데 여기서 항상 -march=core-avx2 옵션을 사용해서 AVX2가 지원되지 않는 머신에서도 AVX2를 사용해서 gcc가 빌드하도록 하는 걸로 보였다. 다른 코드 빌드할 때는 문제가 없는데 유독 ACL library에서만 이런 문제가 나서 보다 보니 아무래도 Makefile이 잘못된 듯 하다.</description>
    </item>
    
    <item>
      <title>Astri vRAN</title>
      <link>https://cychong47.github.io/post/2018/astri-vran/</link>
      <pubDate>Sun, 15 Jul 2018 12:03:00 +0900</pubDate>
      
      <guid>https://cychong47.github.io/post/2018/astri-vran/</guid>
      <description>Toward 5G RAN virtualization by Intel and Astri
http://astri.oeg
Flexible architecture Modular PHY processing architectures  PDCP Split MAC/PHY Split - HARQ processing in RRU(How???) Lower PHY Split - High FB overhead but smallest packet latency.  Good for JT and JR for COMP Good for Massive MIMO and Ultra low-latency communication(Why?)   FAPI based MAC/PHY communication  L1 adaptation layer for MAC/PHY split (and Lower PHY Split?)    MAC/PHY split in one CPU  MAC/PHY split in one machine but netrwork based MAC/PHY communication over OVS   Virtual Cell  A group of physical cells form a Virtual Cell which does not require HO between the physical cells.</description>
    </item>
    
    <item>
      <title>DPDK IPv4 reassembly</title>
      <link>https://cychong47.github.io/post/2016/dpdk-ipv4-reassembly/</link>
      <pubDate>Thu, 24 Mar 2016 15:03:55 +0900</pubDate>
      
      <guid>https://cychong47.github.io/post/2016/dpdk-ipv4-reassembly/</guid>
      <description>rte_ipv4_frag_reassemble_packet()
 ip_frag_find()  기존에 존재하는 flow면 해당 flow를 저장한 entry 정보를(ip_frag_pkt *pkg) 신규 flow인 경우 해당 신규 flow를 저장할 신규 혹은 재사용된 entry를 return함 추가할 수 있는 통계  신규 flow? 기존 flow에 정상 추가 기존 flow에 비정상 추가(기존 flow가 timeouted) 이도 저도 아닌 상황(할당 실패) LRU entry free tbl-&amp;gt;max_entries tbl-&amp;gt;use_entries   return  기존 존재하는 flow, 신규 할당한 flow entry 혹은 NULL 만일 NULL을 return하면 현재 수신한 mbuf를 death row에 추가한다.</description>
    </item>
    
    <item>
      <title>DPDK new mbuf 사용 주의사항</title>
      <link>https://cychong47.github.io/post/2016/header-length-in-mbuf/</link>
      <pubDate>Sun, 06 Mar 2016 08:22:24 +0900</pubDate>
      
      <guid>https://cychong47.github.io/post/2016/header-length-in-mbuf/</guid>
      <description>l2_len, l3_len, l4_len 등을 사용하는 라이브러리가 존재함
 reassembly Tx checksum offload  Reassembly rte_ipv6_frag_reassemble_packet(), rte_ipv4_frag_reassemble_packet() Incoming mbuf should have its l2_len and l3_len fields setup correctly.
L4 checksum HW offloading To use hardware L4 checksum offload, the user needs to
 fill l2_len and l3_len in mbuf set the flags PKT_TX_TCP_CKSUM, PKT_TX_SCTP_CKSUM or PKT_TX_UDP_CKSUM set the flag PKT_TX_IPV4 or PKT_TX_IPV6 calculate the pseudo header checksum and set it in the L4 header (only for TCP or UDP).</description>
    </item>
    
    <item>
      <title>KNI가 buffer를 free 하는 방법</title>
      <link>https://cychong47.github.io/post/2016/how_kni_free_mbuf/</link>
      <pubDate>Sun, 06 Mar 2016 08:17:58 +0900</pubDate>
      
      <guid>https://cychong47.github.io/post/2016/how_kni_free_mbuf/</guid>
      <description>DPDK to KNI RX KNI는 rx_q로부터 mbuf를 수신한 후 data_len 크기의 skb를 할당하여 데이터를 복사한 후 netif_rx를 호출한다. 그러므로 mbuf는 KNI kernel module까지만 사용되고, 커널 networking stack에서는 사용되지는 않는다.
kni_net.c의 kni_net_rx_normal() 함수가 DPDK application으로부터 mbuf를 받아 커널에 전달하는 함수인데 실제 함수는 batch processing을 위해 한번에 여러 개의 패킷을 rx_q로부터 읽어 처리하도록 구현되어 있다.
아래는 하나의 패킷에 대해 수행되는 코드를 간략화 한 것이다(예외 처리 부분도 제외)
num_rx = kni_fifo_get(kni-&amp;gt;rx_q, (void **)va, num_rx); kva = (void *)va[i] - kni-&amp;gt;mbuf_va + kni-&amp;gt;mbuf_kva; len = kva-&amp;gt;data_len; data_kva = kva-&amp;gt;buf_addr + kva-&amp;gt;data_off - kni-&amp;gt;mbuf_va + kni-&amp;gt;mbuf_kva; skb = dev_alloc_skb(len + 2); /* Align IP on 16B boundary */ skb_reserve(skb, 2); memcpy(skb_put(skb, len), data_kva, len); skb-&amp;gt;dev = dev; skb-&amp;gt;protocol = eth_type_trans(skb, dev); skb-&amp;gt;ip_summed = CHECKSUM_UNNECESSARY; /* Call netif interface */ netif_rx(skb); /* Update statistics */ kni-&amp;gt;stats.</description>
    </item>
    
    <item>
      <title>fd.io</title>
      <link>https://cychong47.github.io/post/2016/fd-io/</link>
      <pubDate>Sat, 13 Feb 2016 01:29:43 +0900</pubDate>
      
      <guid>https://cychong47.github.io/post/2016/fd-io/</guid>
      <description>2016년 2월 11일 공개된 CISCO 주도의 프로젝트. 무려 2002년부터 개발한 것으로 현재 버전은 3번째 revision이라고 한다.
간만에 dpdk.org mailing list에 들어갔다 가장 최근에 올라온 글 제목이 눈에 띄었다.
 [dpdk-dev] [dpdk-announce] new project using DPDK - FD.io Vincent JARDIN
 &amp;ldquo;new project&amp;rdquo;?
그래서 내용을 봤더니 이게 다 였다는
A new project using DPDK is available, http://FD.io said FiDo You can clone it from: http://gerrit.fd.io/ Best regards, Vincent 그래도 첫 번째 링크를 따라가 보니 화려하다.</description>
    </item>
    
    <item>
      <title>DPDK NIC 초기화</title>
      <link>https://cychong47.github.io/post/2016/dpdk_nic_init/</link>
      <pubDate>Tue, 09 Feb 2016 14:54:21 +0900</pubDate>
      
      <guid>https://cychong47.github.io/post/2016/dpdk_nic_init/</guid>
      <description>constructor attribute http://phoxis.org/2011/04/27/c-language-constructors-and-destructors-with-gcc/
constructor attribute을 가진 함수는 main 함수를 실행하기 전에 호출한다.
예제 (출처)
#include &amp;lt;stdio.h&amp;gt; void begin (void) __attribute__((constructor)); void end (void) __attribute__((destructor)); int main (void) { printf (&amp;quot;\nInside main ()&amp;quot;); } void begin (void) { printf (&amp;quot;\nIn begin ()&amp;quot;); } void end (void) { printf (&amp;quot;\nIn end ()\n&amp;quot;); } 실행하면
In begin () Inside main () In end () DPDK DPDK의 경우 device driver들을 모두 constructor attirbute을 사용해서 main 함수 전에 호출되록 한다.</description>
    </item>
    
    <item>
      <title>SR-IOV and DPDK</title>
      <link>https://cychong47.github.io/post/2016/sriov-and-dpdk/</link>
      <pubDate>Sun, 07 Feb 2016 23:40:26 +0900</pubDate>
      
      <guid>https://cychong47.github.io/post/2016/sriov-and-dpdk/</guid>
      <description>SR-IOV and DPDK
Accelerating the NFV Data Plane : SR-IOV and DPDK… in my own words 를 읽고 요약
Before HW assisted Virtualisation SR-IOV 전까지는 VMM이 패킷 송수신에 매번 개입해야 했음.
 1st interrupt from NIC to VMM 2nd interrupt from VMM to VM  Intel VMDq Only one interrupt from NIC to VM as each VM has its own Rx queue.
SR-IOV  SR-IOV : Standard IO memory Memory Management Unit from Intel(VT-d) and AMD(IOV) Virtual Function - Limited, lightweight, PCIe resource and a dedicated Tx/Rx packet queue Interrupt 부담이 없다고 하는데 왜?</description>
    </item>
    
    <item>
      <title>DPDK based applications</title>
      <link>https://cychong47.github.io/post/2016/dpdk_based_apps/</link>
      <pubDate>Sun, 24 Jan 2016 14:59:18 +0900</pubDate>
      
      <guid>https://cychong47.github.io/post/2016/dpdk_based_apps/</guid>
      <description>2016.02.10 기준
 DPDK-dump TRex - Realistic traffic generator
git-hub - trex-core, trex-doc, trex-profiles, trex-qt-gui Packet-journey git-hub FD.io Fast Data Path DPDK-nginx DPDK-pktgen DPDK-ODP TCP/IP stack for DPDK  </description>
    </item>
    
    <item>
      <title>DPDK QAT example  빌드하기</title>
      <link>https://cychong47.github.io/post/2016/build_dpdk_qat/</link>
      <pubDate>Fri, 01 Jan 2016 10:57:39 +0900</pubDate>
      
      <guid>https://cychong47.github.io/post/2016/build_dpdk_qat/</guid>
      <description>Download dpdk-2.2.0.tar.gz Refer http://dpdk.org/download
wget http://dpdk.org/browse/dpdk/snapshot/dpdk-2.2.0.tar.gz  Download qat_mux Refer https://01.org/packet-processing/intel®-quickassist-technology-drivers-and-patches
wget https://01.org/sites/default/files/page/qatmux.l.2.5.0-80.tgz  Getting Started Guide 문서도 받아 둔다.
wget https://01.org/sites/default/files/page/330750-004_qat_gsg.pdf  Configure DPDK export RTE_SDK=/home/cychong/work/dpdk-2.2.0 export RTE_TARGET=x86_64-native-linuxapp-gcc make config T=$RTE_TARGET O=$RTE_TARGET make  Configure QAT Ubuntu (14.04) 기준으로 몇 개 패키지를 설치해야 QAT를 빌드할 수 있는데 나름 기본적인 패키지들이라 그냥 설치해 놓으면 좋을 듯.
sudo apt-get install zlib1g-dev sudo apt-get install libssl-dev  적당한 위치에 풀면 되는데 ~/work/qat에 압축을 푼 경우를 기준으로 정리</description>
    </item>
    
    <item>
      <title>CISCO Cloud Service Platform 2100</title>
      <link>https://cychong47.github.io/post/2015/cisco-cloud-service-platform-2100/</link>
      <pubDate>Mon, 23 Nov 2015 14:24:35 +0900</pubDate>
      
      <guid>https://cychong47.github.io/post/2015/cisco-cloud-service-platform-2100/</guid>
      <description>Data sheet (PDF)
 Some Cisco virtual network services that use the DPDK include Cisco Cloud Services Router (CSR) 1000V, Cisco Virtual Mobile Packet Core software, and Cisco IOS® XR 9000v virtual router. Supporte CISCO appliances  Cisco Cloud Services Router (CSR) 1000V virtual router Cisco Virtual Adaptive Security Appliance (ASAv) Cisco Prime™ Data Center Network Manager (DCNM) Cisco Virtual Network Analysis Module (vNAM) Cisco Virtual Security Gateway (VSG) for Cisco Nexus® 1000V Switch deployments Cisco Virtual Supervisor Module (VSM) for Cisco Nexus 1000V Switch deployments   1U 2 CPU, each has 8 core Ivy Bridge(E5-2630 v3) REST API It uses REST API and NETCONF protocol for north-bound management and orchestration (MANO) tools.</description>
    </item>
    
    <item>
      <title>DPDK IP reassembly example</title>
      <link>https://cychong47.github.io/post/2015/dpdk-ip-reassembly-example/</link>
      <pubDate>Tue, 17 Nov 2015 13:49:57 +0900</pubDate>
      
      <guid>https://cychong47.github.io/post/2015/dpdk-ip-reassembly-example/</guid>
      <description>TAILQ_HEAD(ip_pkt_list, ip_frag_pkt); /**&amp;lt; @internal fragments tailq */  자료 구조체 Fragment 관리용 table struct rte_ip_frag_tbl *frag_tbl;  locking 없이 IP reassembly를 수행할 단위(통상 core)로 한 개씩 만든다. 즉 하나의 core가 여러 rx queue를 처리하더라도 하나의 frag_tbl만 가지면 된다.
아래 rte_ip_frag_table_create()함수를 이용해서 생성한다.
struct rte_ip_frag_death_row death_row core별로 갖는 death_row. IP reassembly를 호출한 후 해당 함수내에서 free할 mbuf를 이 리스트에 담아줌.
main loop에서 reassembly작업 후 rte_ip_frag_free_death_row()함수를 호출해 reassembly에 실패한 mbuf를 free함
IP_MAX_FRAG_NUM defines the maximum fragments of one reassembly.</description>
    </item>
    
    <item>
      <title>Running DPDK on VMware Fusion</title>
      <link>https://cychong47.github.io/post/2015/running-dpdk-on-vmware-fusion/</link>
      <pubDate>Tue, 20 Oct 2015 14:19:05 +0900</pubDate>
      
      <guid>https://cychong47.github.io/post/2015/running-dpdk-on-vmware-fusion/</guid>
      <description>VirtualBox supports emulated e1000 NIC for VM while VMware fusion does not. VMware Fusion&amp;rsquo;s VM setting does not support configuring of NIC HW type. The NIC HW is PCnet32 which is not supported by DPDK.
However, we can change NIC HW type by editing VM configuration file directly.
Refer : How to emulate 10 Gbps NIC in a VMware Fusion VM
Edit vmx file to VMX file is in where vmware image located</description>
    </item>
    
    <item>
      <title>TRex - DPDK based traffic generator</title>
      <link>https://cychong47.github.io/post/2015/trex-dpdk-based-traffic-generator/</link>
      <pubDate>Mon, 12 Oct 2015 13:11:14 +0900</pubDate>
      
      <guid>https://cychong47.github.io/post/2015/trex-dpdk-based-traffic-generator/</guid>
      <description>DPDK Userspace in Dublin 2015에서 발표
Stageful traffic generator
특징  Generate traffic based on templates of real, captured flows No TCP/IP stack Up to 200Gbps with standard server hardware Low cost 1RU (C220M UCS-1RU) Cisco internal DPDK, ZMQ, Python libs Virtualization(vmxnet3/e1000) ~20Gbps per core Generate flow templates Support 1K templates Yaml based traffic profile  GUI GUI which monitors real-time properties of TRex - min/max/average latency, jitter
Python 연동 Code https://github.</description>
    </item>
    
    <item>
      <title>DPDK Coding style</title>
      <link>https://cychong47.github.io/post/2015/dpdk-coding-style/</link>
      <pubDate>Thu, 21 May 2015 14:51:37 +0900</pubDate>
      
      <guid>https://cychong47.github.io/post/2015/dpdk-coding-style/</guid>
      <description>출처 : DPDK mailing list
Coding Style Description This document specifies the preferred style for source files in the DPDK source tree. It is based on the Linux Kernel coding guidelines and the FreeBSD 7.2 Kernel Developer&amp;rsquo;s Manual (see man style(9)), but was heavily modified for the needs of the DPDK.
General Guidelines The rules and guidelines given in this document cannot cover every situation, so the following general guidelines should be used as a fallback:</description>
    </item>
    
    <item>
      <title>Intel Embedded Tech Forum 2014</title>
      <link>https://cychong47.github.io/post/2014/intel-embedded-tech-forum-2014/</link>
      <pubDate>Tue, 09 Dec 2014 10:38:08 +0900</pubDate>
      
      <guid>https://cychong47.github.io/post/2014/intel-embedded-tech-forum-2014/</guid>
      <description>Small Cell  Big Cell 256 user 이하를 small cell로 정의 mini-CRAN Paris Hill SOC을 이용하는 경우 RRH에서 LTE/3G DSP+DFE 까지 처리하고 Ethernet으로 IA core로 전달. Altiostar 구조와 유사한 듯  Wifi 와 3G/4G까지 지원하는 차세대 SOC 2/4/8 core까지 지원   Aricent와 협업하여 L1/L2/L3 Protocol stack 개발  ONP  Red Rock Canyon  Las Vegas에서 30분 가량 걸리는 거리   Switch와 NIC 통합 PCI-e를 지원해서 NIC없이 Xeon을 직접 연결할 수 있음.</description>
    </item>
    
    <item>
      <title>R.I.P OVDK</title>
      <link>https://cychong47.github.io/post/2014/r-i-p-ovdk/</link>
      <pubDate>Thu, 27 Nov 2014 14:47:38 +0900</pubDate>
      
      <guid>https://cychong47.github.io/post/2014/r-i-p-ovdk/</guid>
      <description>며칠 밖에 보지 않았지만, 그래도 내용을 분석해 보려고 했던 OVDK인데, 오늘 기사를 보니 Intel에서 공식적으로 OVDK의 개발 중단을 발표했단다.
Intel Dead-Ends Its Fork of Open vSwitch
Data path(Fast path)를 커널 모듈에서 처리하는 OVS를 fork해서 DPDK를 이용해서 user space에 Fast Path를 만들려고 했는데 그러다 보니 역시 계속해서 발전하는 OVS의 기능을 수용하기 부담스러웠나 보다. 더군다나 OVS에서도 experimental feature이긴 하지만 DPDK를 이용하는 코드도 있다고 하니.
내년 초에 나올 다음 버전 OVS에 공식 기능으로 들어가길 기대한다고.</description>
    </item>
    
    <item>
      <title>DPDK Summit 2014 Videos</title>
      <link>https://cychong47.github.io/post/2014/dpdk-summit-2014-videos/</link>
      <pubDate>Wed, 19 Nov 2014 13:59:07 +0900</pubDate>
      
      <guid>https://cychong47.github.io/post/2014/dpdk-summit-2014-videos/</guid>
      <description> Application Performance Tuning and Future Optimizations in DPDK by Venky Venkatesan DPDK in a Virtual World by Bhavesh Davda Rashmin Patel High Performance Networking Leveraging the DPDK and the Growing Community by Thomas Monj alon Multi Socket Ferrari for NFV by Laszlo Vadkerti Andras Kovacs Lightning Fast IO with PacketDirect by Gabriel Silva A High Performance vSwitch of the User by the User for the User by Yoshihiro Nakajima Is It Time to Revisit the IP Stack in the Linux Kernel and KVM by Jun Xu Closing Remarks by Tim ODriscoll  </description>
    </item>
    
    <item>
      <title>DPDK on VirtualBox</title>
      <link>https://cychong47.github.io/post/2014/dpdk-on-virtualbox/</link>
      <pubDate>Tue, 23 Sep 2014 15:37:21 +0900</pubDate>
      
      <guid>https://cychong47.github.io/post/2014/dpdk-on-virtualbox/</guid>
      <description>VirtualBox에 DPDK 설치하기 참고
VirtualBox 설치하기 통상적인 절차대로 VirtualBox를 설치하고, Ubuntu 14.04 LTS 설치한다. DPDK는 32bit와 64bit를 모두 지원하지만 64비트를 사용하는 것이 좋다. Application에 따라 많은 양의 Memory를 사용할 수도 있으므로.
NIC 카드 추가 VirtualBox가 지원하는 NIC에 Intel 82540EM과 82545EM이 있다. 둘 다 DPDK에서 지원하는 1G NIC이다. 이 중에서 82545EM 카드를 2개 추가한다.
VirtualBox의 Guest OS를 종료시킨 상태에서 환경 설정에서 Network &amp;gt; Adapter 항목에서 Adapter 2, Adapter 3를 활성화시킨다.
그 결과 총 3개의 NIC이 설치되었다.</description>
    </item>
    
    <item>
      <title>SDN expert group 세미나 - Play with DPDK</title>
      <link>https://cychong47.github.io/post/2014/sdn-expert-group-semina-play-with-dpdk/</link>
      <pubDate>Thu, 21 Aug 2014 12:11:18 +0900</pubDate>
      
      <guid>https://cychong47.github.io/post/2014/sdn-expert-group-semina-play-with-dpdk/</guid>
      <description>세미나 내용  Controller &amp;ndash;(OpenFlow)&amp;ndash; ovs-switchd &amp;ndash;(netlink)&amp;ndash; Datapath Datapath is in the kernel space OVDK move the kernel based OVS to user space.  ovs-switched talk to OVDK with UDP   기존 OVDK는 port별 task handler(각각 별도의 core에서 동작)  그 결과 많은 core 필요 WR 이야기처럼 VM간 혹은 VM과 외부와의 통신을 담당하는 OVS용으로 많은 core를 사용하면 실제로 VM이 사용할 수 있는 core 개수가 줄어들어 문제   virtIO 사용시 VM에서 동작하는 application이 kernel stack의 필요한 경우 결국 OVDK와 VM내 커널 space간 copy가 필요함  최신 버전에서는 VM에서도 KNI based virtIO를 이용하도록 개선함.</description>
    </item>
    
  </channel>
</rss>