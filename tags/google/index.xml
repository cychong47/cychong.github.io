<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>google on Keep calm and write something</title><link>https://cychong47.github.io/tags/google/</link><description>Recent content in google on Keep calm and write something</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sun, 01 Oct 2017 08:34:30 +0900</lastBuildDate><atom:link href="https://cychong47.github.io/tags/google/index.xml" rel="self" type="application/rss+xml"/><item><title>Google's Load Balancer</title><link>https://cychong47.github.io/post/2017/googles-load-balancer/</link><pubDate>Sun, 01 Oct 2017 08:34:30 +0900</pubDate><guid>https://cychong47.github.io/post/2017/googles-load-balancer/</guid><description>Maglev Google( https://research.google.com/pubs/pub44824.html ) Used in Google Cloud since 2008 Scalable load balancer Consistent hashing Connection Tracking Scale-out model backed by router&amp;rsquo;s ECMP Bypass kernel space for performance. Support connection persistence Network Architecture DNS - Routers - Maglevs - Service EndPoints. One service is served by one or more VIPs DNS returns VIP considering geolocation and load of location One VIP is served by multiple Maglevs Router use ECMP to select one Maglev One VIP is mapped to multiple Service EndPoints Maglev select Service EndPoint by seletion algorithm and connection tracking table Maglev use GRE to send incoming packet to Service EndPoint or another Maglev Send to IP fragment to another special Maglev servers Use only 3-tuple for IP fragment Each Service EndPoint use Direct Server Return(DSR) Maglev Controller Responsible for VIP announcement with BGP Check health status of forwarder If forwarder is not headthy, withdraw all VIP announcements Forwarder Each VIP has one or multiple backend pools(BP) BP contain physical IP address of the Service EndPoint Each BP has specific health checking methods - depends on the service requirement(just reachability or more) Config Manager parse and update configuration of forwarder&amp;rsquo;s behavior based on the Config Objects Sharding Sharding of Maglev enables service isolation - new service or QoS Backend Selection Consistent Hashing distribute loads Record selection in LOCAL connection tracking table Connection tracking table is not shared with another Maglev Does not guarantee consistency on Maglev or Service EndPoint Changes(add/delete) For different traffic type TCP SYN : select Backend and record it in connection tracking table TCP non-SYN : lookup connection tracking table 5-tuple : (maybe) lookup connection tracking table and select backend if not found Consistent Hashing If Maglev is added or removed, router select different Maglev for the exsiting session - ECMP is changed If one Maglev&amp;rsquo;s local connection tracking table is overflowed, it will lose previous selection To resolve this issues, Synchronize local connection tracking table between Maglevs -&amp;gt; overhead, overhead, overhead Consistent hashing for minimize disruption in member changes Maglev hashing - load balancing and minimal disruption on member changes reference Maglev: A Fast and Reliable Software Network Load Balancer Consistent Hashing The Simple Magic of Consistent Hashing</description></item><item><title>Espresso - Google's peering edge architecture</title><link>https://cychong47.github.io/post/2017/espresso-googles-peering-edge-architecture/</link><pubDate>Wed, 12 Apr 2017 00:32:57 +0900</pubDate><guid>https://cychong47.github.io/post/2017/espresso-googles-peering-edge-architecture/</guid><description>Google Fellow Amin Vahdat,
“Early on, we realized that the network we needed to support our services did not exist and could not be bought,”
Espresso makes Google cloud faster, more available and cost effective by extending SDN to the public internet
network should be treated as a large-scale distributed system leveraging the same control infrastructure we developed for Google’s compute and storage systems Four pillars on Google&amp;rsquo;s SDN strategy Jupiter: Google employed SDN principles to build Jupiter, a data center interconnect capable of supporting more than 100,000 servers.</description></item><item><title>Googlegeist vs. SCI</title><link>https://cychong47.github.io/post/2016/googlegeist_vs_sci/</link><pubDate>Sat, 02 Jan 2016 02:44:51 +0900</pubDate><guid>https://cychong47.github.io/post/2016/googlegeist_vs_sci/</guid><description>SCI 결과를 개선하기 위해 실질적으로 이뤄지는 노력이 안 보인다는 것. 노력한다해도 그건 관리자와 비관리자가 함께 노력해야 하는 일일텐데(관리자나 회사에 대한 불만</description></item><item><title>Culture should be setup first</title><link>https://cychong47.github.io/post/2015/culture/</link><pubDate>Sun, 13 Dec 2015 13:44:24 +0900</pubDate><guid>https://cychong47.github.io/post/2015/culture/</guid><description>from Google Work Rules 2006년에 구글에 입사. 72년 생 구글 임직원 나이 평균에 비하면 많지만, 그래도 비슷한 덩치의 국내 기업의 인사 담당자와 비교하면. 하긴 구글을 국내 (</description></item></channel></rss>