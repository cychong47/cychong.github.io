<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>gpu on Keep calm and write something</title><link>https://cychong47.github.io/tags/gpu/</link><description>Recent content in gpu on Keep calm and write something</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 02 Nov 2020 08:56:16 +0900</lastBuildDate><atom:link href="https://cychong47.github.io/tags/gpu/index.xml" rel="self" type="application/rss+xml"/><item><title>Chapter 2. CUDA Memory Management</title><link>https://cychong47.github.io/post/2020/2020-11-02-chapter-2.-cuda-memory-management/</link><pubDate>Mon, 02 Nov 2020 08:56:16 +0900</pubDate><guid>https://cychong47.github.io/post/2020/2020-11-02-chapter-2.-cuda-memory-management/</guid><description>2. CUDA Memory Management Most of the applicationâ€™s performance will be bottlecked by memory-related constraints GPU RAM BW : 900GB/s (DDR3 ?) NV Visual Profiler Global memory is a staging area where all of the date gets copied from CPU memory. Global Memory(device memory) is visible to all of the threads in the kernel and also visible to CPu. Coalesced vs. uncoalesced global memory access coalesced global memory access : Sequential memory access is adjacent Warp - Warp is a unit of thread scheduling/execution in SMs.</description></item><item><title>Chapter 1. Introduction to CUDA Programming</title><link>https://cychong47.github.io/post/2020/2020-11-02-chapter-1-introduction-to-cuda-programming/</link><pubDate>Mon, 02 Nov 2020 08:55:34 +0900</pubDate><guid>https://cychong47.github.io/post/2020/2020-11-02-chapter-1-introduction-to-cuda-programming/</guid><description>1. Introduction to CUDA Programming CPU Architecture is optimized for low latency accessing while GPU architecture is optimized for data parallel throughput compution CPU hide latency of data by frequently stroring used data in caches and utilize the temporal locality In CUDA, the execution unit is a warp not a thread. Context switching is happens between the warps and not threads. GPU has lots of registers, all the thread context switching information is already present in the registers.</description></item><item><title>NVIDIA vRAN Solution</title><link>https://cychong47.github.io/post/2020/nvidia-vran-solution/</link><pubDate>Wed, 20 May 2020 15:01:44 +0900</pubDate><guid>https://cychong47.github.io/post/2020/nvidia-vran-solution/</guid><description>https://devblogs.nvidia.com/building-accelerated-5g-cloudran-at-the-edge/
Mellanox ConnectX-6 Dx SmartNIC exceeds stringent industry-standard timing specifications for eCPRI-based RANs by ensuring clock accuracy of 16ns or less
5T for 5G enables packet-based, ethernet RANs to provide precise time-stamping of packets for delivering highly accurate time references to 5G fronthaul and backhaul networks.
5T-for-5G, or time-triggered transmission technology for telco
https://news.developer.nvidia.com/new-real-time-smartnic-technology-5t-for-5g/
Real-time transmission hardware acceleration: 5T-for-5G simplifies time synchronization and data transmission across servers, GPUs, radios, and baseband units in wireless network rollouts, making 5G rollouts easier and more efficient.</description></item></channel></rss>