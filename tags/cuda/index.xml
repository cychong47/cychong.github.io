<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>CUDA on Keep calm and write something</title><link>https://cychong47.github.io/tags/cuda/</link><description>Recent content in CUDA on Keep calm and write something</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 02 Nov 2020 08:56:16 +0900</lastBuildDate><atom:link href="https://cychong47.github.io/tags/cuda/index.xml" rel="self" type="application/rss+xml"/><item><title>Chapter 2. CUDA Memory Management</title><link>https://cychong47.github.io/post/2020/2020-11-02-chapter-2.-cuda-memory-management/</link><pubDate>Mon, 02 Nov 2020 08:56:16 +0900</pubDate><guid>https://cychong47.github.io/post/2020/2020-11-02-chapter-2.-cuda-memory-management/</guid><description>2. CUDA Memory Management Most of the applicationâ€™s performance will be bottlecked by memory-related constraints GPU RAM BW : 900GB/s (DDR3 ?) NV Visual Profiler Global memory is a staging area where all of the date gets copied from CPU memory. Global Memory(device memory) is visible to all of the threads in the kernel and also visible to CPu. Coalesced vs. uncoalesced global memory access coalesced global memory access : Sequential memory access is adjacent Warp - Warp is a unit of thread scheduling/execution in SMs.</description></item><item><title>Chapter 1. Introduction to CUDA Programming</title><link>https://cychong47.github.io/post/2020/2020-11-02-chapter-1-introduction-to-cuda-programming/</link><pubDate>Mon, 02 Nov 2020 08:55:34 +0900</pubDate><guid>https://cychong47.github.io/post/2020/2020-11-02-chapter-1-introduction-to-cuda-programming/</guid><description>1. Introduction to CUDA Programming CPU Architecture is optimized for low latency accessing while GPU architecture is optimized for data parallel throughput compution CPU hide latency of data by frequently stroring used data in caches and utilize the temporal locality In CUDA, the execution unit is a warp not a thread. Context switching is happens between the warps and not threads. GPU has lots of registers, all the thread context switching information is already present in the registers.</description></item></channel></rss>